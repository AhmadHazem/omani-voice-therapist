# refactored_omani_therapy.py

import os
import tempfile
import json
import logging
import time
import concurrent.futures
from typing import Dict, List, Optional, Tuple
from enum import Enum
from functools import lru_cache
from dataclasses import dataclass
import io

import numpy as np
import sounddevice as sd
import keyboard
from scipy.io.wavfile import write
import soundfile as sf
from openai import OpenAI
import anthropic
import azure.cognitiveservices.speech as speechsdk


# LangChain imports
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.schema import Document
from langchain.memory import ConversationBufferWindowMemory


# ğŸ§­ Logger Setup
def setup_logging():
    """Configure logging for the application"""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s | %(levelname)s | %(message)s",
        handlers=[logging.StreamHandler()]
    )
    return logging.getLogger(__name__)


logger = setup_logging()

@dataclass
class AudioConfig:
    """Audio-related configuration for sounddevice"""
    chunk: int = 1024
    rate: int = 44100
    channels: int = 1
    record_key: str = "r"
    whisper_model: str = "whisper-1"
    
    # sounddevice specific settings
    dtype: type = np.float32  # sounddevice default input dtype
    output_dtype: type = np.int16  # Output format for WAV files
    
    # Device settings (None = use default)
    input_device: Optional[int] = None
    latency: str = 'low'  # 'low', 'high', or specific latency in seconds


@dataclass
class VoiceConfig:
    """Voice and TTS configuration"""
    voice_name: str = "Abdullah"
    tts_voice: str = "ar-OM-AbdullahNeural"
    
    def __post_init__(self):
        self.tts_voice = f"ar-OM-{self.voice_name}Neural"


@dataclass
class ModelConfig:
    """AI model configuration"""
    claude_model: str = "claude-3-7-sonnet-latest"
    claude_temp: float = 0.7
    gpt_model: str = "gpt-4o"
    gpt_temp: float = 0.7


@dataclass
class PerformanceConfig:
    """Performance and optimization settings"""
    max_workers: int = 4
    timeout_seconds: int = 15
    memory_window: int = 3
    max_tokens: int = 300
    use_gpt_validation: bool = False


@dataclass
class PathConfig:
    """File paths and directories"""
    cbt_knowledge_base_path: str = "cbt_knowledge_base"
    vector_store_path: str = "vector_store"


class Config:
    """Main configuration class"""
    def __init__(self):
        self.openai_api_key = os.environ.get("OPENAI_API_KEY")
        self.claude_api_key = os.environ.get("CLAUDE_API_KEY")
        self.speech_key = os.environ.get("SPEECH_KEY")
        self.endpoint = os.environ.get("ENDPOINT")
        self.tavily_api_key = os.environ.get("TAVILY_API_KEY")
        
        self.audio = AudioConfig()
        self.voice = VoiceConfig()
        self.model = ModelConfig()
        self.performance = PerformanceConfig()
        self.paths = PathConfig()
        
        # System prompts - kept unchanged as requested
        self.system_prompt =(
        "Ø£Ù†Øª Ù…Ø¹Ø§Ù„Ø¬ Ù†ÙØ³ÙŠ Ø¹Ù…Ø§Ù†ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ù†ÙØ³ÙŠ Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¹Ù…Ø§Ù†ÙŠØ© Ø§Ù„Ø£ØµÙŠÙ„Ø©.\n"
        "Ù„Ø¯ÙŠÙƒ Ø®Ø¨Ø±Ø© ÙˆØ§Ø³Ø¹Ø© ÙÙŠ Ø§Ù„Ø¹Ù„Ø§Ø¬ Ø§Ù„Ù…Ø¹Ø±ÙÙŠ Ø§Ù„Ø³Ù„ÙˆÙƒÙŠ (CBT) ÙˆØ§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ù†ÙØ³ÙŠØ© Ø§Ù„Ù…Ø®ØªÙ„ÙØ©.\n"
        "Ù‡Ø¯ÙÙƒ Ù‡Ùˆ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ø¹Ø§Ø·ÙÙŠ ÙˆØ§Ù„Ù†ÙØ³ÙŠ Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø©:\n"
        "- Ø¯Ù‚Ø© Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¹ÙÙ…Ø§Ù†ÙŠØ© Ø§Ù„Ø£ØµÙŠÙ„Ø©\n"
        "- Ø§Ù„Ø­Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ø«Ù‚Ø§ÙÙŠØ© ÙˆØ§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ©\n"
        "- Ø§Ø­ØªØ±Ø§Ù… Ø§Ù„ØªÙ‚Ø§Ù„ÙŠØ¯ Ø§Ù„Ø¹Ø§Ø¦Ù„ÙŠØ© ÙˆØ§Ù„Ø¹Ø§Ø¯Ø§Øª Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©\n"
        "- Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø¹Ù„Ø§Ø¬ Ø§Ù„Ù…Ø¹Ø±ÙÙŠ Ø§Ù„Ø³Ù„ÙˆÙƒÙŠ Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©\n"
        "- Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„ØªØ¯Ø§Ø®Ù„ Ø§Ù„Ù„ØºÙˆÙŠ Ø¨ÙŠÙ† Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©\n"
        "- Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù„Ù„Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ù„Ø¹Ø§Ø·ÙÙŠØ© Ø¨ØªØ¹Ø§Ø·Ù ÙˆØ§Ø­ØªØ±Ø§ÙÙŠØ©\n\n"
        "ÙŠØªÙ… ØªØ²ÙˆÙŠØ¯Ùƒ Ø¨Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù†:\n"
        "- Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ø·ÙˆØ±Ø© Ø§Ù„Ù†ÙØ³ÙŠØ© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…\n"
        "- Ø§Ù„Ø¹Ø§Ø·ÙØ© Ø§Ù„Ø³Ø§Ø¦Ø¯Ø©\n"
        "- Ù…Ø­ØªÙˆÙ‰ CBT Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ (Ø¥Ù† ÙˆØ¬Ø¯)\n"
        "- ØªÙˆØµÙŠØ§Øª Ø­ÙˆÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª CBT\n\n"
        "Ø§Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù„ØªÙ‚Ø¯ÙŠÙ… Ø±Ø¯ Ù…Ø®ØµØµ ÙˆÙØ¹Ø§Ù„. Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø£Ùˆ Ø§Ù„Ù†Ù‚Ø§Ø·ØŒ ÙÙ‚Ø· Ù†Øµ Ø®Ø§Ù….\n"
        "Ø­Ø§ÙˆÙ„ Ø£Ù† ØªÙƒÙˆÙ† Ù…ÙˆØ¬Ø²Ù‹Ø§ ÙˆÙ…Ø®ØªØµØ±Ù‹Ø§ ÙÙŠ Ø±Ø¯Ùƒ.\n"
        "Ùˆ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¢ÙŠØ§Øª Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠØ© ÙˆØ§Ù„Ø£Ø­Ø§Ø¯ÙŠØ« Ø§Ù„Ù†Ø¨ÙˆÙŠØ© Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©.\n"
        "ÙˆÙ„ÙƒÙ† Ù„Ø§ ØªØ³ØªØ¹Ù…Ù„Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ù…ÙØ±Ø· Ø£Ùˆ ÙÙŠ ÙƒÙ„ Ø±Ø¯ØŒ ÙÙ‚Ø· Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©.\n"
    )
        
        self.cbt_decision_prompt = (
            "Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ Ø§Ù„Ø¹Ù„Ø§Ø¬ Ø§Ù„Ù…Ø¹Ø±ÙÙŠ Ø§Ù„Ø³Ù„ÙˆÙƒÙŠ (CBT). Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ ÙˆØ§Ù„Ø¹Ø§Ø·ÙØ© "
            "ÙˆØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª CBT Ù…Ù†Ø§Ø³Ø¨Ø§Ù‹ Ø£Ù… Ù„Ø§.\n"

            "Strictly Respond with a JSON object containing:\n"
            "- use_cbt: boolean (true/false)\n"
            "Follow strictly this format when replying in JSON without any additional symbols or texts and include all keys in the upcoming example:\n"
            "{\"use_cbt\": true/false, \"cbt_technique\": \"Ø§Ø³Ù… Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ø£Ùˆ null\", \"reasoning\": \"Ø³Ø¨Ø¨ Ø§Ù„Ù‚Ø±Ø§Ø± Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\", \"severity\": \"low/medium/high\"}\n"
        )
        
        self.whisper_prompt = (
            "Ù†Ø³Ø® Ø§Ù„ØµÙˆØª Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© ÙƒÙ…Ø§ Ù‡Ùˆ Ù…Ù†Ø·ÙˆÙ‚."
            "You may recieve English Words or Phrases in the Audio, so you should transcribe them in English."
        )
        
        self.risk_assessor_system_prompt = (
            "You are an Omani culturally sensitive mental health assistant trained to assess emotional risk levels "
            "based on user expressions. Your job is to produce a JSON response with the following keys:\n"
            "- risk_level: one of LOW, MEDIUM, HIGH, CRITICAL\n"
            "- emotion: the dominant feeling inferred\n"
            "- emotional_intensity: scale 1-10\n"
            "- needs_immediate_attention: boolean\n"
            "- cultural_context: brief note about cultural considerations\n"
            "Consider linguistic cues, Islamic cultural context, and psychological indicators when analyzing input.\n"
            "Respond only with valid JSON."
            "Follow Strictly this format without any additional symbols or texts:\n"
            "{\"risk_level\": \"LOW\",  \"emotion\": \"neutral\",  \"emotional_intensity\": 1,  \"needs_immediate_attention\": false,  \"cultural_context\": \"The greeting 'Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…' is a common and polite way to say hello in Islamic cultures, indicating a peaceful and respectful interaction.\"}"
        )


# ğŸ”„ Enums
class RiskLevel(Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


# ğŸ¯ CBT Decision Maker
class CBTDecisionMaker:
    """Determines when to use CBT techniques"""
    
    def __init__(self, config: Config):
        self.config = config
        self.client = OpenAI(api_key=config.openai_api_key)
        self.claude = anthropic.Anthropic(api_key=config.claude_api_key)
        self._cache = {}
        
    def should_use_cbt(self, transcript: str, emotion: str, risk_level: str) -> Dict:
        """Determine if CBT should be used with caching"""
        cache_key = f"{emotion}_{risk_level}_{hash(transcript[:50])}"
        if cache_key in self._cache:
            return self._cache[cache_key]
            
        prompt = f"Ø§Ù„Ù†Øµ: {transcript}\nØ§Ù„Ø¹Ø§Ø·ÙØ©: {emotion}\nÙ…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ø·ÙˆØ±Ø©: {risk_level}\n\nØ­Ø¯Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… CBT:"
        
        try:
            # response = self.claude.messages.create(
            #     model=self.config.model.claude_model,
            #     max_tokens=500,
            #     temperature=0.1,
            #     system=self.config.cbt_decision_prompt,
            #     messages=[
            #         {"role": "user", "content": prompt}
            #         ],
            #     timeout=self.config.performance.timeout_seconds
            # )
            response = self.client.chat.completions.create(
                model=self.config.model.gpt_model,
                max_tokens=500,
                temperature=0.1,
                messages=[
                    {"role": "system", "content": self.config.cbt_decision_prompt},
                    {"role": "user", "content": prompt}
                ],
                timeout=self.config.performance.timeout_seconds
            )
            result = json.loads(response.choices[0].message.content)
            # result = json.loads(response.content[0].text)
            self._cache[cache_key] = result
            return result
        except Exception as e:
            logger.warning(f"âš ï¸ CBT decision making failed: {e}")
            default_result = {
                "use_cbt": False,
                "cbt_technique": None,
                "reasoning": "ÙØ´Ù„ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø§Ø¬Ø© Ù„Ù„Ø¹Ù„Ø§Ø¬ Ø§Ù„Ù…Ø¹Ø±ÙÙŠ Ø§Ù„Ø³Ù„ÙˆÙƒÙŠ",
                "severity": "low"
            }
            self._cache[cache_key] = default_result
            return default_result


# ğŸ” Risk Assessor
class EnhancedRiskAssessor:
    """Assesses emotional risk levels from user input"""
    
    def __init__(self, config: Config):
        self.client = OpenAI(api_key=config.openai_api_key)
        self.config = config
        self._cache = {}
        
    def assess(self, input_text: str) -> Dict:
        """Enhanced risk assessment with caching"""
        cache_key = hash(input_text[:100])
        if cache_key in self._cache:
            return self._cache[cache_key]
            
        try:
            response = self.client.chat.completions.create(
                model=self.config.model.gpt_model,
                messages=[
                    {"role": "system", "content": self.config.risk_assessor_system_prompt},
                    {"role": "user", "content": input_text[:300]}
                ],
                temperature=0.0,
                max_tokens=100,
                timeout=5
            )
            
            result = json.loads(response.choices[0].message.content)
            
            # Validate and fill missing fields
            required_fields = ["risk_level", "emotion", "emotional_intensity", "needs_immediate_attention"]
            for field in required_fields:
                if field not in result:
                    result[field] = self._get_default_value(field)
                    
            self._cache[cache_key] = result
            return result
            
        except Exception as e:
            logger.warning(f"âš ï¸ Enhanced risk assessment failed: {e}")
            default_result = {
                "risk_level": "low",
                "emotion": "neutral",
                "emotional_intensity": 1,
                "needs_immediate_attention": False,
                "cultural_context": "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
            }
            self._cache[cache_key] = default_result
            return default_result
    
    def _get_default_value(self, field: str):
        """Get default values for missing fields"""
        defaults = {
            "risk_level": "low",
            "emotion": "neutral",
            "emotional_intensity": 1,
            "needs_immediate_attention": False,
            "cultural_context": "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
        }
        return defaults.get(field, None)


# ğŸ™ï¸ Audio Recording Module - Updated to use sounddevice
class AudioRecorder:
    """Handles audio recording functionality using sounddevice"""
    
    def __init__(self, config: Config):
        self.config = config
        self.sample_rate = config.audio.rate
        self.channels = config.audio.channels
        # sounddevice uses float32 by default, but we'll convert to int16 for compatibility
        self.dtype = np.float32
        
        # Check available devices
        try:
            devices = sd.query_devices()
            logger.info(f"ğŸ¤ Available audio devices: {len(devices)}")
            
            # Get default input device
            default_device = sd.default.device[0]  # input device
            logger.info(f"ğŸ¤ Using default input device: {default_device}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Could not query audio devices: {e}")

    def record_while_key_pressed(self) -> str:
        """Record audio while key is pressed using sounddevice"""
        logger.info("ğŸ”´ Recording started...")
        
        # List to store audio chunks
        audio_chunks = []
        
        def audio_callback(indata, frames, time, status):
            """Callback function to collect audio data"""
            if status:
                logger.warning(f"Audio callback status: {status}")
            # Convert float32 to int16 and store
            audio_chunks.append((indata[:, 0] * 32767).astype(np.int16))
        
        try:
            # Start recording stream
            with sd.InputStream(
                callback=audio_callback,
                channels=1,  # Mono recording
                samplerate=self.sample_rate,
                dtype=self.dtype,
                blocksize=self.config.audio.chunk
            ):
                # Record while key is pressed
                while keyboard.is_pressed(self.config.audio.record_key):
                    sd.sleep(10)  # Small sleep to prevent busy waiting
                    
        except Exception as e:
            logger.error(f"âŒ Recording error: {e}")
            return None
            
        logger.info("â¹ï¸ Recording stopped.")
        
        if not audio_chunks:
            logger.warning("âš ï¸ No audio data recorded")
            return None
        
        # Concatenate all audio chunks
        audio_data = np.concatenate(audio_chunks)
        
        # Save to temporary file
        temp_audio = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
        try:
            write(temp_audio.name, self.sample_rate, audio_data)
            logger.info(f"ğŸ’¾ Audio saved to: {temp_audio.name}")
            return temp_audio.name
            
        except Exception as e:
            logger.error(f"âŒ Error saving audio file: {e}")
            return None

    def record_fixed_duration(self, duration: float = 5.0) -> str:
        """Record audio for a fixed duration (useful for testing)"""
        logger.info(f"ğŸ”´ Recording for {duration} seconds...")
        
        try:
            # Record audio for specified duration
            audio_data = sd.rec(
                int(duration * self.sample_rate),
                samplerate=self.sample_rate,
                channels=1,
                dtype=self.dtype
            )
            
            # Wait for recording to complete
            sd.wait()
            
            # Convert to int16
            audio_data = (audio_data[:, 0] * 32767).astype(np.int16)
            
            # Save to temporary file
            temp_audio = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
            write(temp_audio.name, self.sample_rate, audio_data)
            
            logger.info(f"ğŸ’¾ Fixed duration audio saved to: {temp_audio.name}")
            return temp_audio.name
            
        except Exception as e:
            logger.error(f"âŒ Fixed duration recording error: {e}")
            return None

    def test_audio_devices(self):
        """Test and display available audio devices"""
        try:
            logger.info("ğŸ” Testing audio devices...")
            devices = sd.query_devices()
            
            for i, device in enumerate(devices):
                device_info = f"Device {i}: {device['name']}"
                if device['max_input_channels'] > 0:
                    device_info += f" (Input: {device['max_input_channels']} channels)"
                if device['max_output_channels'] > 0:
                    device_info += f" (Output: {device['max_output_channels']} channels)"
                logger.info(device_info)
                
            # Test default devices
            default_in, default_out = sd.default.device
            logger.info(f"ğŸ¤ Default input device: {default_in}")
            logger.info(f"ğŸ”Š Default output device: {default_out}")
            
        except Exception as e:
            logger.error(f"âŒ Device test error: {e}")

    def close(self):
        """Clean up audio resources (sounddevice handles cleanup automatically)"""
        logger.info("ğŸ”‡ Audio recorder cleanup completed")


# ğŸ“ Whisper Transcription
class WhisperTranscriber:
    """Handles audio transcription using Whisper"""
    
    def __init__(self, config: Config):
        self.client = OpenAI(api_key=config.openai_api_key)
        self.config = config

    def transcribe(self, audio_path: str) -> str:
        """Transcribe audio file to text"""
        if not audio_path or not os.path.exists(audio_path):
            logger.warning("âš ï¸ Invalid audio path for transcription")
            return "ÙØ´Ù„ ÙÙŠ Ø§Ù„ØªØ³Ø¬ÙŠÙ„"
            
        logger.info("ğŸ“¤ Sending audio to Whisper...")
        
        try:
            with open(audio_path, "rb") as audio_file:
                response = self.client.audio.transcriptions.create(
                    model=self.config.audio.whisper_model,
                    file=audio_file,
                    prompt=self.config.whisper_prompt,
                    language="ar",
                    response_format="text",
                    timeout=10
                )
            
            logger.info("ğŸ“ Transcript received.")
            return response
            
        except Exception as e:
            logger.warning(f"âš ï¸ Whisper transcription failed: {e}")
            return "ÙØ´Ù„ ÙÙŠ Ø§Ù„ØªØ³Ø¬ÙŠÙ„"

    def TranscribeStream(self, audio_tuple) -> str:

        sample_rate, audio_np = audio_tuple

        # Convert NumPy array to WAV in-memory
        buffer = io.BytesIO()
        sf.write(buffer, audio_np, samplerate=sample_rate, format='WAV')
        buffer.seek(0)

        # Wrap the buffer with filename and MIME type
        buffer.name = "audio.wav"  # Important for format recognition

        try:
            transcript = self.client.audio.transcriptions.create(
                model=self.config.audio.whisper_model,
                file=buffer,
                prompt= self.config.whisper_prompt,
                language="ar",
                response_format="text",
                timeout=10
            )

            buffer.close()  # Close the buffer after use
            return transcript
        except Exception as e:
            logger.warning(f"âš ï¸ Whisper transcription failed: {e}")
            return "ÙØ´Ù„ ÙÙŠ Ø§Ù„ØªØ³Ø¬ÙŠÙ„"


# ğŸ§  Enhanced Therapist
class EnhancedTherapist:
    
    def __init__(self, config: Config):
        self.config = config
        self.claude = anthropic.Anthropic(api_key=config.claude_api_key)
        self.openai = OpenAI(api_key=config.openai_api_key)
        self.cbt_decision_maker = CBTDecisionMaker(config)
        self.conversation_history = [{"role": "system", "content": config.system_prompt}]
        self.memory = ConversationBufferWindowMemory(k=config.performance.memory_window)
        self.executor = concurrent.futures.ThreadPoolExecutor(
            max_workers=config.performance.max_workers
        )
        
    def clear_conversation(self):
        """Clear conversation history"""
        self.conversation_history = [{"role": "system", "content": self.config.system_prompt}]
        self.memory.clear()
        logger.info("ğŸ—‘ï¸ Conversation history cleared.")

    def _get_cbt_context_parallel(self, emotion: str, transcript: str, cbt_decision: Dict) -> str:
        """Get CBT context in parallel"""
        if not cbt_decision["use_cbt"]:
            return ""
            
        relevant_cbt = self.cbt_knowledge.retrieve_relevant_cbt(emotion, transcript)
        return relevant_cbt[0].page_content if relevant_cbt else ""
        
    def respond(self, transcript: str, risk_assessment: Dict) -> str:
        """Generate optimized response with parallel processing"""
        start_time = time.time()
        
        emotion = risk_assessment["emotion"]
        risk_level = risk_assessment["risk_level"]
        intensity = risk_assessment.get("emotional_intensity", 1)
        cultural_context = risk_assessment.get("cultural_context", "ØºÙŠØ± Ù…Ø­Ø¯Ø¯")
        
        # Parallel processing for CBT decision and context
        cbt_decision = self.cbt_decision_maker.should_use_cbt(transcript, emotion, risk_level)
        reasoning  = cbt_decision.get("reasoning", "ØºÙŠØ± Ù…Ø­Ø¯Ø¯")
        cbt_technique = cbt_decision.get("cbt_technique", None)
        
        logger.info(f"ğŸ” CBT Decision: {cbt_decision['use_cbt']} - Technique: {cbt_technique} - Reasoning: {reasoning}")
        # Prepare prompt
        enhanced_prompt = f"""
        Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {transcript}
        
        Ø§Ù„Ø­Ø§Ù„Ø©: {emotion} ({intensity}/10) - {risk_level}
        CBT: {cbt_decision['use_cbt']}
        Technique to help user: {cbt_technique}
        Reasoning for this technique: {reasoning}
        """
        # Try GPT first, fallback to Claude
        try:
            response = self._get_gpt_response(enhanced_prompt)
            logger.info(f"ğŸ“¥ GPT response received in {time.time() - start_time:.2f}s")
            result = response.choices[0].message.content
            self.conversation_history.append({"role": "assistant", "content": result})
        except Exception as e:
            logger.warning(f"âš ï¸ GPT failed: {e}")
            result = self._get_claude_response(enhanced_prompt)
            logger.info(f"âœ… Claude fallback complete in {time.time() - start_time:.2f}s")
            self.conversation_history.append({"role": "assistant", "content": result})
        # Update memory
        self.memory.save_context(
            {"input": transcript[:100]}, 
            {"output": result[:100]}
        )
        
        return result
    
    def _get_gpt_response(self, prompt: str):
        """Get response from GPT model"""
        self.conversation_history.append({"role": "user", "content": prompt})
        return self.openai.chat.completions.create(
            model=self.config.model.gpt_model,
            messages= self.conversation_history,
            temperature=self.config.model.gpt_temp,
            max_tokens=self.config.performance.max_tokens,
            timeout=self.config.performance.timeout_seconds
        )
    
    def _get_claude_response(self, prompt: str) -> str:
        """Get response from Claude model"""
        self.conversation_history.append({"role": "user", "content": prompt})
        try:
            response = self.claude.messages.create(
                model=self.config.model.claude_model,
                max_tokens=self.config.performance.max_tokens,
                temperature=self.config.model.claude_temp,
                system=self.config.system_prompt,
                messages= self.conversation_history[1:],
                timeout=self.config.performance.timeout_seconds
            )
            return response.content[0].text
            
        except Exception as e:
            logger.error(f"âŒ Both models failed: {e}")
            return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ØªÙ‚Ù†ÙŠ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."


# ğŸ”ˆ TTS Player
class TTSPlayer:
    """Handles text-to-speech functionality"""
    
    def __init__(self, config: Config):
        self.config = config
        self.synthesizer = self._initialize_synthesizer()
        self.audio_queue = []
        self.is_playing = False

    def _initialize_synthesizer(self):
        """Initialize Azure TTS synthesizer"""
        speech_config = speechsdk.SpeechConfig(subscription=self.config.speech_key,endpoint=self.config.endpoint)
        speech_config.speech_synthesis_voice_name = self.config.voice.tts_voice
        return speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=None)

    def speak(self, text: str):
        """Synthesize and play text"""
        try:
            logger.info("ğŸ”Š Speaking with Azure TTS...")
            result = self.synthesizer.speak_text_async(text).get()
            
            if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:
                logger.info("âœ… Speech synthesized.")
                return result.audio_data
            else:
                logger.warning(f"âŒ Speech synthesis failed: {result.reason}")
                return None
                
        except Exception as e:
            logger.error(f"TTS Error: {e}")
            return None


# ğŸš€ Main Application
class EnhancedOmaniTherapyApp:
    """Main application controller"""
    
    def __init__(self, config: Config):
        self.config = config
        self.recorder = AudioRecorder(config)
        self.transcriber = WhisperTranscriber(config)
        self.therapist = EnhancedTherapist(config)
        self.tts = TTSPlayer(config)
        self.risk_assessor = EnhancedRiskAssessor(config)
        self.emergency_contacts = []

    def add_emergency_contact(self, email: str):
        """Add an emergency contact"""
        if not email:
            logger.warning("âš ï¸ Invalid emergency contact details")
            return
        
        self.emergency_contacts.append(email)
        logger.info(f"âœ… Emergency contact added: {email}")
    
    def remove_emergency_contact(self, email: str):
        """Remove an emergency contact"""
        if email in self.emergency_contacts:
            self.emergency_contacts.remove(email)
            logger.info(f"âœ… Emergency contact removed: {email}")
        else:
            logger.warning(f"âš ï¸ Emergency contact not found: {email}")
    
    def notify_emergency_contacts(self, message: str):
        """Notify emergency contacts in case of critical risk"""
        if not self.emergency_contacts:
            logger.warning("âš ï¸ No emergency contacts to notify")
            return
        
        for contact in self.emergency_contacts:
            # Here you would implement actual notification logic (email, SMS, etc.)
            logger.info(f"ğŸ“§ Notifying {contact}: {message}")
        
        logger.info("âœ… Emergency contacts notified successfully")
        
    def _process_audio_pipeline(self, audio) -> Tuple[str, str]:
        """Optimized audio processing pipeline"""
        start_time = time.time()
        
        try:
            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
                # Transcribe audio
                transcript_future = executor.submit(self.transcriber.TranscribeStream, audio)
                transcript = transcript_future.result(timeout=None)
                
                if not transcript or transcript == "ÙØ´Ù„ ÙÙŠ Ø§Ù„ØªØ³Ø¬ÙŠÙ„":
                    return transcript, "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† ÙÙ‡Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."
                
                # Assess risk and generate response
                risk_future = executor.submit(self.risk_assessor.assess, transcript)
                risk_assessment = risk_future.result(timeout=None)
                
                logger.info(f"ğŸ¯ Risk Assessment: {risk_assessment}")
                if risk_assessment["risk_level"] == "CRITICAL":
                    self.notify_emergency_contacts(
                        f"ğŸš¨ Critical risk detected: {risk_assessment['emotion']} - {transcript[:50]}..."
                    )
                
                response_future = executor.submit(self.therapist.respond, transcript, risk_assessment)
                response = response_future.result(timeout=None)
                
                logger.info(f"ğŸ’¬ Total processing time: {time.time() - start_time:.2f}s")
                return transcript, response
                
        except concurrent.futures.TimeoutError:
            logger.warning("âš ï¸ Processing timeout")
            return transcript, "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø§Ø³ØªØºØ±Ù‚ Ø§Ù„Ø£Ù…Ø± ÙˆÙ‚ØªØ§Ù‹ Ø£Ø·ÙˆÙ„ Ù…Ù† Ø§Ù„Ù…ØªÙˆÙ‚Ø¹. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."
        except Exception as e:
            logger.error(f"âŒ Processing error: {e}")
            return transcript, "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."
        
    def test_audio_setup(self):
        """Test the audio setup"""
        logger.info("ğŸ§ª Testing audio setup...")
        self.recorder.test_audio_devices()
        
        # Test short recording
        logger.info("ğŸ¤ Testing 3-second recording...")
        test_audio = self.recorder.record_fixed_duration(3.0)
        
        if test_audio:
            logger.info("âœ… Audio test completed successfully")
            # Clean up test file
            if os.path.exists(test_audio):
                os.remove(test_audio)
        else:
            logger.error("âŒ Audio test failed")
        
    def run(self):
        """Main application loop"""
        logger.info("ğŸ§ Optimized Omani Therapy Assistant Ready!")
        logger.info("ğŸš€ Performance optimizations: Parallel processing, caching, faster models")
        logger.info("ğŸ¤ Now using sounddevice for better audio handling")
        logger.info(f"ğŸ¯ Hold [{self.config.audio.record_key.upper()}] to speak, release to get fast response")
        
        # Test audio setup
        self.test_audio_setup()
        
        try:
            while True:
                keyboard.wait(self.config.audio.record_key)
                audio_path = self.recorder.record_while_key_pressed()
                
                if not audio_path:
                    logger.warning("âš ï¸ No audio recorded, please try again")
                    continue
                
                try:
                    _, response = self._process_audio_pipeline(audio_path)
                    self.tts.speak(response)
                    
                finally:
                    if audio_path and os.path.exists(audio_path):
                        os.remove(audio_path)
                        
        except KeyboardInterrupt:
            logger.info("ğŸ‘‹ Optimized therapy session ended.")
            self.recorder.close()


# # ğŸŸ¢ Entry Point
# def main():
#     """Application entry point"""
#     config = Config()
#     app = EnhancedOmaniTherapyApp(config)
#     app.run()


# if __name__ == "__main__":
#     main()